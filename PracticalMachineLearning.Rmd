---
title: "Practical Machine Learning Assignment"
author: "Jenifer Jones"
date: "October 19, 2015"
output: html_document
---

<h1><b>Introduction</b></h1> 

This project uses a data set of personal activity monitoring values based 
on wearable devices.  The data set represents how well a particular acitivity, 
in this case the Unilateral Dumbbell Biceps Curl was done.  Measurements were 
taken from accelerometers on the belt, forearm, arm and dumbbell.  Each 
user was asked to perform the excercise in five different ways, which map 
to the five different values of the predicted variable.  These are exactly 
according to the specification (A), throwing elbows to the front (B), lifting
the dumbbell only half way (C), lowering the dumbbell only halfway (D) and 
throwing the hips to the front (E).  This data was sourced from <link>http://groupware.les.inf.puc-rio.br/har</link>.  

The purpose of this project was to build a model or models based on the dataset
with the intent to predict the classe variable.  Multiple models were created 
as explained in the content below, each with different levels of accuracy and 
error.  

<h1><b>Data Load </b></h1> 

The first steps taken were to set the working directory for the project, load the R packages required for analysis and load the modeling and evaluation data sets provided. Based on a preliminary look at the data (not included in this markdown file for purposes of brevity) it was found that there are certain types of missing values within the dataset which need to be accounted for during the data load, this was done within the parameters of the "read.csv"" command.  

```{r, results='hide', warning=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(caret)
library(randomForest)
library(e1071)
library(lattice)
library(corrplot)
library(rpart)
library(adabag)
library(gbm)

set.seed(13)
setwd(("C:\\Users\\jeniferjones\\SkyDrive\\Coursera\\Data Scientists Toolbox\\COURSE_8_PRACT_ML\\Project"))
modeling<-read.csv("pml-training.csv", na.strings= c("NA", "#DIV/0!"))
evaluation<-read.csv("pml-testing.csv", na.strings= c("NA", "#DIV/0!"))
```

<h1><b>Exploratory Data Analysis </b></h1> 

Taking a look at the structure of the data I found that there are 19,622 observations across 160 variables. Since I noticed a number of "NA" values during the initial data load a summary of the columns was completed to see how prevalent these values were.  

```{r results='hide'}
str(modeling)
summary(modeling)
```

It was noted that for many variables the majority of the observations were "NA" so these columns were removed from the dataset for analysis purposes.  For consistency
in modeling these columns were removed from the evaluation set also. This was done by counting all of the "NA" values for all of the variables (with the exception of the predicted variable). Based on this those variables with a count greater than zero were removed from the modeling and subsequently evaluation data. 

```{r}
Empty_Count <- sapply(1:dim(modeling)[2],function(x)sum(is.na(modeling[,x])))
Removal_List <- which(Empty_Count>0)
modeling<- modeling[, -Removal_List]
evaluation<- evaluation[, -Removal_List]
```

I performed another summary on the data and found that I still had 19,622 observations, but now across only 60 variables.  The first column is just a unique identifier of our record so that will not be relevant to the data analysis and will be removed.  Additionally, the time stamp and user information will also be removed as these are not expected to contribute to the prediction of the activity classification. 

```{r}
modeling<- modeling[, -c(1:8)]
evaluation<- evaluation[, -c(1:8)]
```

To complete the exploratory analysis a correlation matrix was generated to show where there may be variables that are highly correlated.  The axis descriptions have been removed for readability.  This shows that there is high correlation between the early variables in the data set and some other pockets of high correlation throughout, mostly concentrated in areas that you would expect to be similar based on measurement type. 

```{r}
modeling_corr<-cor(modeling[, 1:51])
corrplot(modeling_corr, method="color", cl.pos="n", tl.pos="n")
```

<h1><b>Model Building</b></h1> 

Now that the training and testing datasets are prepared the predictive modeling can begin.  The variable we are trying to predict is a feature variable which makes this 
a classification problem. 

I decided to use three different predictive modeling methods and will use the model
with the best outcome to submit for the secondary assignment.  The three methods are:

      1. Basic Decision Tree (using the rpart package)
      2. Bagging (using the adabag package) = Used to reduce any over-fitting that
      may have occurred in the original model 
      3. Random Forest (using the randomForest package) = Uses similar boot strapping
      technique as bagging, however random feature sampling is also done at each split
      within the tree.  
      
For all of the models I have created training and testing data sets based on a 70/30
split of the data. The seed has also been set in the first code chunk so that the results are reproducible.

```{r, echo=FALSE}

partition<-createDataPartition(y=modeling$classe, p=0.7, list=FALSE)
training<-modeling[partition, ]
testing<-modeling[-partition, ]

```


<h3><b>Model 1 - Basic Decision Tree</b> </h3>

The basic decision tree will give us a simple model that can then be visualized 
in a plot in a way that a user will understand how the model makes its predictions.  
This ease of understanding often comes with a trade-off in the models accuracy. 
Below is the model, output of the decision tree and the confusion matrix which highlights the models accuracy.  

```{r}
set.seed(13)
model_1<-rpart(as.factor(classe)~., data=training, method="class")

par(xpd=TRUE)
plot(model_1, compress=TRUE)
title(main="Basic Decision Tree")
text(model_1)

model_1.pred<-predict(model_1, testing, type="class")
confusionMatrix(model_1.pred, testing$classe)

```

The accuracy of this model based on the confusion matrix is: <b>.6683</b>.  The 
corresponding out of sample error is <b>0.3317</b> or <b>33%</b>.  This is good baseline to start
from as it is a simple model and the expectation is that the more complex models
will perform better. 

<h3><b>Model 2 - Modeling using Bagging </b> </h3>

By using bagging we are adding some complexity into our model by developing 
multiple trees which are then aggregated to make a better set of predictions. 
This method creates multiple predictions for the observations and then uses the 
most often occurrence as the predicted variable.  

Below is the model and the confusion matrix which highlights the models accuracy.  

```{r}
model_2 <- bagging(classe ~ ., data=training, mfinal=50)
model_2.pred <- predict(model_2, newdata=testing, newmfinal=50)

confusionMatrix(as.factor(model_2.pred$class), testing$classe)
```

The accuracy of this model based on the confusion matrix is: <b>0.7259</b> The 
corresponding out of sample error is <b>.2741</b> or <b>27.41%</b>.  This is better
than the general decision tree, but the expectation is that the next two models
will decrease the out of sample error even further. 

<h3><b>Model 3 - Random Forest </b> </h3>

The final model that I developed was a Random Forest Model and was the one 
that I expected to have the highest accuracy.  This is based on the fact 
that while it also creates multiple predictions for each of the observations
it uses a random feature selection approach when developing the tree and uses
the strongest selection at each branch.  

I created two random forest models, one using the caret package with 5-fold 
cross validation specified and the second using the randomForest package selecting 100 
trees and using a tuned mtry value.  

Below are the models and the confusion matrices which highlight their accuracy.  

```{r}
model_3_caret<-train(classe~., data=training, method="rf", 
               trControl=trainControl(method="cv", number=3), 
               prox=TRUE, allowParallel=TRUE)
model_3_caret.pred<-predict(model_3_caret, testing)
confusionMatrix(model_3_caret.pred, testing$classe)
```

The accuracy of this model based on the confusion matrix is: <b>0.9944</b> The 
corresponding out of sample error is <b>.0056</b> or <b>0.56%</b>.

```{r}


# Use the tuneRF function to determine an ideal value for the mtry parameter
mtry <- tuneRF(training[,1:51], training[,52], mtryStart=1, ntreeTry=100, stepFactor=2, improve=0.05,trace=TRUE, plot=TRUE, doBest=FALSE)

# The ideal mtry value was found to be 8

model_3_rf <- randomForest(as.factor(classe) ~ ., data=training, importance=TRUE, 
                        ntree=100, mtry=8, allowParallel=TRUE)
model_3_rf.pred <- predict(model_3_rf, testing, type="response")
confusionMatrix(model_3_rf.pred, testing$classe)
```

The accuracy of this model based on the confusion matrix is: <b>0.9952</b> The 
corresponding out of sample error is <b>.0048</b> or <b>0.48%</b>.  The following plot
shows the top 15 features that are most important to the analysis. 

```{r}
varImpPlot(model_3_rf, sort = TRUE, n.var = 15, main = "Random Forest Features")
```

<h1><b>Conclusion</b></h1>

Based on the models created the model with the highest accuracy is the Random Forest
model with a tuned mtry parameter and 100 trees.  However, the literature about this
model states that it can cause overfitting so for the next steps in the assignment the 
output from the caret package model will be used. Based on this model I expect the 
out of sample error to be .0056.  

For future modeling I would try principal component analysis based on the information
shown on the correlation matrix which highlights some of the strongly correlated 
features.

<h1><b>Secondary Submission</b></h1>

The second part of this assignment was to apply the model to a set of 20 observations
provided without a classe variable assigned.  The following code was used to create 
a function to copy each of the predictions from the text set into separate files to be submitted. 


```{r}

Assignment_Predictions <- predict(model_3_caret,evaluation)
Assignment_Predictions

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(Assignment_Predictions)
```

Based on this output all of the predicted values were correct when loaded to the 
course submission page.